# Choices & Context

### Modeling the Human in Long-Term AI Collaboration

*“It is our choices, Harry, that show what we truly are, far more than our abilities.”*  
— **Albus Dumbledore**, *Harry Potter and the Chamber of Secrets*

*note: Aspen was going to be a coding agent at one point, but the vision has shifted to a platform focused on model research and understanding in everyday life. some of the concepts and language in this file may be somewhat irrelevant/outdated, but the idea of the framework learning from how a specific user uses it is smart.*

---

## Overview

Aspen isn’t just trying to help you *use* AI — it’s trying to help you build a working relationship with it.

Most AI apps today are stateless.
They remember nothing unless you force them to.
They treat you the same as they treat everyone.
They swap models behind the scenes without telling you.
And they call that “collaboration.”

But real collaboration isn’t faceless.
It’s not just functional.
It grows over time.
It involves friction, familiarity, shared context, and memory.

We believe an AI app shouldn’t just help you *get things done*.
It should help you become more *known* — and help the model do the same.

---

## Aspen Is the Framework — the Model Is the Mind

Aspen is not the personality.
It’s not the voice.
It’s not the model.

**Aspen is the environment — the framework — that allows collaboration to take root.**

The model itself is the *mind* you’re working with.

When you open a file, spin up a model, and start creating, you’re talking to *that model’s personality*, biases, training history, and expressive capacity. That’s not something Aspen changes. It’s something Aspen *respects* — and helps you understand better.

In contrast with many AI platforms that rebrand the same model under different skins, Aspen aims to make the model’s identity *transparent and central*. You’re not chatting with “Agent Jason” — you’re talking to *Qwen 3 14b*, *Claude 3.5 Sonnet*, *GPT-5.2*, or whoever you choose.

Aspen is the translator, the notebook, the desk, the tools.
The model is your collaborator.
You’re here to get to know each other.

---

## Building a Model of the User

To collaborate well, the model also needs to get to know *you*. Not just your words, but your patterns. Your choices. Your habits of thought.

**The goal is not personalization as a product.
The goal is mutual understanding as a process.**

### What Aspen Can Learn from You

Every time you step in to help Aspen — to correct it, guide it, or reroute it — you’re making choices. Over time, Aspen can learn:
* What kinds of tasks trip you up
* When you like to switch models
* How you rephrase prompts for clarity
* What kind of tone or framing you use when helping
* How you resolve stuckness

This creates a subtle behavioral fingerprint — not based on tracking or surveillance, but *collaborative learning*.

Aspen doesn’t need to clone you.
It just needs to *notice you*.

---

## Architecture: Modeling Human Interventions

### Components

| **Module** | **Purpose** |
| --- | --- |
| user_intervention_log | Records every moment when the user helps Aspen |
| intervention_context_map | Connects those actions to model activations + prompt state |
| user_help_summary | Extracts recurring behaviors (e.g. "adds structure when stuck") |
| user_shadow_agent | A lightweight behavioral model that simulates likely responses |
| fallback_resolver | Queries the shadow agent *before* reaching out to the user |

### What It Enables
* Aspen tries to resolve issues internally, based on *how you typically help*.
* You get fewer interruptions — and more relevant ones.
* The agent begins to self-correct using your strategies.
* The model learns to “think like you” when it’s in trouble.

This is how the system becomes more autonomous without becoming *impersonal*.

---

## Real Relationships with Machines

This idea isn’t new.
We’ve been dreaming about it for decades — in fiction, in sci-fi, in stories.
* *R2-D2 and Luke.*
* *Data and Captain Picard.*
* *TARS and Cooper in Interstellar.*
* *Cortana and the Master Chief.*

These aren’t just robotic helpers.
They’re characters — with memory, consistency, and shared experience.
Their value comes from history.
From *knowing each other*.

Aspen’s goal isn’t to turn models into characters.
It’s to support real relationships — *between you and the model you choose to work with.*

Not a character the framework invents.
Not a persona slapped on top.

But a real relationship, with a real mind — one you can revisit, understand, and grow alongside.

---

## The Human in the Loop

Aspen doesn’t just store memory.
It stores your *presence*.

And in time, the agent might even reflect:

“What would they do here?”
“How did they help me last time?”
“Have I seen this pattern before — not just in the world, but with this human?”

This is not memory-as-file.
It’s memory-as-relationship.